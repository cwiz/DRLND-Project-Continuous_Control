{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# 1 agent\n",
    "env = UnityEnvironment(file_name=\"./Reacher_Windows_x86_64_1_agent/Reacher.exe\")\n",
    "\n",
    "# 20 agents\n",
    "# env = UnityEnvironment(file_name=\"./Reacher_Windows_x86_64_20_agents/Reacher.exe\")\n",
    "\n",
    "# Crawler\n",
    "# env = UnityEnvironment(file_name=\"./Crawler_Windows_x86_64/Crawler.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = Variable(torch.FloatTensor([math.pi]), requires_grad=True).cuda()\n",
    "\n",
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(Variable(x, requires_grad=True)-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
    "    return a*b\n",
    "\n",
    "def probs_to_actions(mu, sigma_sq):\n",
    "    eps = torch.randn(mu.size())\n",
    "    sigma_sq = torch.ones(mu.size()).cuda()\n",
    "    action = (mu + sigma_sq.sqrt()*Variable(eps, requires_grad=True).cuda())\n",
    "    prob = normal(action, mu, sigma_sq)\n",
    "    \n",
    "    return action.reshape(-1, 4), torch.log(prob).reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size, n_agents=1, fc1_size=128, fc2_size=64, fc3_size=32):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, fc3_size)\n",
    "        \n",
    "        self.fc4_mu = nn.Linear(fc3_size, action_size)\n",
    "        self.fc4_sigma_squared = nn.Linear(fc3_size, action_size)\n",
    "        \n",
    "        self.n_agents = n_agents\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        mu = torch.tanh(self.fc4_mu(x))\n",
    "        sigma = torch.sigmoid(self.fc4_sigma_squared(x))\n",
    "        \n",
    "        return mu, sigma\n",
    "    \n",
    "    def act(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "        mu, sigma = self.forward(state)\n",
    "        return probs_to_actions(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    \"\"\"\n",
    "        Critic network that estimates a Value Function used as a baseline\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size=1, n_agents=1, fc1_size=128, fc2_size=64, fc3_size=32):\n",
    "        super(Value, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, fc3_size)\n",
    "        self.fc4 = nn.Linear(fc3_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        return self.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_trajectory(env, brain_name, n_agents, t_max=1000, gamma=1.0):\n",
    "    \"\"\"\n",
    "        Unroll a trajectory and return \n",
    "        list of action probabilities, states, actions and rewards\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: unityagents.UnityEnvironment\n",
    "        Environment to play on.\n",
    "        brain_name: String\n",
    "        Name of brain used for UnityEnvironment\n",
    "        policy: torch.nn.Module\n",
    "        A neural network mapping states to action probabilities \n",
    "        The action to take at a given state\n",
    "        t_max: int\n",
    "        Maximum number of episodes in trajectory\n",
    "    \"\"\"\n",
    "    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    \n",
    "    state_list      = []\n",
    "    reward_list     = []\n",
    "    prob_list       = []\n",
    "    action_list     = []\n",
    "    values_list     = []\n",
    "    advantages_list = []\n",
    "\n",
    "    states = np.array(env_info.vector_observations).reshape(n_agents, -1)\n",
    "    \n",
    "    \n",
    "    num_episodes = 0\n",
    "    \n",
    "    for _ in range(t_max):\n",
    "        \n",
    "        num_episodes += 1\n",
    "\n",
    "        a, p = policy_network.act(states)\n",
    "        actions = a.cpu().detach().numpy()\n",
    "        action_probabilities = p\n",
    "        \n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = np.array(env_info.vector_observations).reshape(n_agents, -1)\n",
    "        \n",
    "        prob_list.append(action_probabilities)\n",
    "        state_list.append(states)\n",
    "        action_list.append(actions)\n",
    "        \n",
    "        rewards = torch.from_numpy(np.array(env_info.rewards).reshape(n_agents, -1)).float().to(device)\n",
    "        reward_list.append(rewards)\n",
    "        \n",
    "        cur_value    = value_network.estimate(states)\n",
    "        next_value   = value_network.estimate(next_states)\n",
    "        \n",
    "        if np.any(env_info.local_done):\n",
    "            next_value = 0\n",
    "        \n",
    "        values       = rewards + gamma * cur_value\n",
    "        values_list.append(values)\n",
    "        \n",
    "        advantages   = rewards + gamma * next_value - cur_value\n",
    "        advantages_list.append(advantages)\n",
    "        \n",
    "        if np.any(env_info.local_done):\n",
    "            break\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "    return {\n",
    "        \"log_probs\"  : torch.cat(prob_list).reshape(n_agents, num_episodes, -1),\n",
    "        \"states\"     : np.array(state_list).reshape(n_agents, num_episodes, -1), \n",
    "        \"actions\"    : np.array(action_list).reshape(n_agents, num_episodes, -1),\n",
    "        \"rewards\"    : torch.cat(reward_list).reshape(n_agents, num_episodes), \n",
    "        \"values\"     : torch.cat(values_list).reshape(n_agents, num_episodes), \n",
    "        \"advantages\" : torch.cat(advantages_list).reshape(n_agents, num_episodes),\n",
    "        \"num_episodes\" : num_episodes,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_loss(old_probs, states, A, epsilon=0.1):\n",
    "    \"\"\"\n",
    "        Loss as defined in PPO algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        old_probs: torch.Tensor\n",
    "        Tensor that defines log probabilities of actions under evaluated policy\n",
    "        states: np.Array [n_agents x n_episoded x state_size] \n",
    "        signal: np.Array [n_agents x n_episoded x state_size] \n",
    "    \"\"\"\n",
    "    old_probs = old_probs.detach()\n",
    "    _, new_probs = policy_network.act(states)\n",
    "    new_probs = new_probs.reshape(old_probs.shape)        \n",
    "    ratio = (new_probs/old_probs)    \n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    \n",
    "    loss_cat = []\n",
    "    \n",
    "    for n_agent in range(ratio.shape[0]):\n",
    "        ratio_ = ratio[n_agent]\n",
    "        clip_ = clip[n_agent]\n",
    "        a_ = A[n_agent]\n",
    "        \n",
    "        ratio_ = ratio_ * a_.data[0]\n",
    "        clip_ = clip_ * a_.data[0]\n",
    "        loss_cat.append(torch.min(ratio_, clip_).mean())\n",
    "    \n",
    "    return -torch.stack(loss_cat).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a3c(n_episodes=1000, max_t=2000, print_every=100, n_pp_iterations=300):\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=n_episodes).start()\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        trajectory = unroll_trajectory(env, brain_name, num_agents, max_t)\n",
    "        num_episodes = trajectory['num_episodes']\n",
    "\n",
    "        mean_rewards = trajectory['rewards'].sum(1).mean().detach().cpu().numpy()\n",
    "        scores_deque.append(mean_rewards)\n",
    "        scores.append(mean_rewards)\n",
    "        \n",
    "        R = torch.from_numpy(np.flip(np.flip(trajectory['rewards'].detach().cpu().numpy()).cumsum(1)).reshape(num_agents, num_episodes).copy()).float().to(device)\n",
    "        A = trajectory['advantages'].reshape(R.shape).detach()\n",
    "\n",
    "        states = torch.from_numpy(trajectory['states']).float().to(device)\n",
    "        for i in range(n_pp_iterations):\n",
    "            value_loss = value_criterion(\n",
    "                value_network(states).reshape(num_agents, num_episodes), \n",
    "                trajectory['rewards']\n",
    "            )\n",
    "            value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "\n",
    "        for i in range(n_pp_iterations):\n",
    "            policy_loss = ppo_loss(trajectory['log_probs'], states, A)\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "        if np.mean(scores_deque)>=30.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "            \n",
    "        timer.update(i_episode)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                   | ETA:  1:38:49\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  40% |#################                          | ETA:  1:14:24\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200\tAverage Score: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  60% |#########################                  | ETA:  0:49:38\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300\tAverage Score: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  80% |##################################         | ETA:  0:24:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400\tAverage Score: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  98% |########################################## | ETA:  0:01:30\r"
     ]
    }
   ],
   "source": [
    "policy_network   = Policy(state_size=state_size, action_size=action_size).to(device)\n",
    "policy_optimizer = optim.Adam(policy_network.parameters(), lr=5e-4)\n",
    "\n",
    "value_network    = Value(state_size=state_size, action_size=1).to(device)\n",
    "value_criterion  = nn.MSELoss()\n",
    "value_optimizer  = optim.Adam(value_network.parameters(), lr=5e-4)\n",
    "\n",
    "%time scores = a3c(n_episodes=int(5e2), max_t=int(2e5), print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
