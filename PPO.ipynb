{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# 1 agent\n",
    "# env = UnityEnvironment(file_name=\"./Reacher_Windows_x86_64_1_agent/Reacher.exe\")\n",
    "\n",
    "# 20 agents\n",
    "env = UnityEnvironment(file_name=\"./Reacher_Windows_x86_64_20_agents/Reacher.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = Variable(torch.FloatTensor([math.pi]), requires_grad=True).cuda()\n",
    "\n",
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(Variable(x, requires_grad=True)-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
    "    return a*b\n",
    "\n",
    "def probs_to_actions(mu, sigma_sq):\n",
    "    eps = torch.randn(mu.size())\n",
    "    sigma_sq = torch.ones(mu.size()).cuda()\n",
    "    action = (mu + sigma_sq.sqrt()*Variable(eps, requires_grad=True).cuda())\n",
    "    prob = normal(action, mu, sigma_sq)\n",
    "    \n",
    "    return action.reshape(-1, 4), torch.log(prob).reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size, n_agents=1, fc1_size=128, fc2_size=64, fc3_size=64):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, fc3_size)\n",
    "        \n",
    "        self.fc4_mu = nn.Linear(fc3_size, action_size)\n",
    "        self.fc4_sigma_squared = nn.Linear(fc3_size, action_size)\n",
    "        \n",
    "        self.n_agents = n_agents\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        mu = torch.tanh(self.fc4_mu(x))\n",
    "        sigma = torch.sigmoid(self.fc4_sigma_squared(x))\n",
    "        \n",
    "        return mu, sigma\n",
    "    \n",
    "    def act(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "        mu, sigma = self.forward(state)\n",
    "        return probs_to_actions(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    \"\"\"\n",
    "        Critic network that estimates a Value Function used as a baseline\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size=1, n_agents=1, fc1_size=128, fc2_size=64, fc3_size=64):\n",
    "        super(Value, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, fc3_size)\n",
    "        self.fc4 = nn.Linear(fc3_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        return self.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_trajectory(env, brain_name, n_agents, t_max=1000, gamma=1.0):\n",
    "    \"\"\"\n",
    "        Unroll a trajectory and return \n",
    "        list of action probabilities, states, actions and rewards\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: unityagents.UnityEnvironment\n",
    "        Environment to play on.\n",
    "        brain_name: String\n",
    "        Name of brain used for UnityEnvironment\n",
    "        policy: torch.nn.Module\n",
    "        A neural network mapping states to action probabilities \n",
    "        The action to take at a given state\n",
    "        t_max: int\n",
    "        Maximum number of episodes in trajectory\n",
    "    \"\"\"\n",
    "    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    # num_agents = len(env.ps)\n",
    "\n",
    "    state_list      = []\n",
    "    reward_list     = []\n",
    "    prob_list       = []\n",
    "    action_list     = []\n",
    "    values_list     = []\n",
    "    advantages_list = []\n",
    "\n",
    "    states = np.array(env_info.vector_observations).reshape(n_agents, -1)\n",
    "    \n",
    "    for _ in range(t_max):\n",
    "\n",
    "        a, p = policy_network.act(states)\n",
    "        actions = a.cpu().detach().numpy()\n",
    "        action_probabilities = p\n",
    "        \n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = np.array(env_info.vector_observations).reshape(n_agents, -1)\n",
    "        \n",
    "        prob_list.append(action_probabilities)\n",
    "        state_list.append(states)\n",
    "        action_list.append(actions)\n",
    "        \n",
    "        rewards = torch.from_numpy(np.array(env_info.rewards).reshape(n_agents, -1)).float().to(device)\n",
    "        reward_list.append(rewards)\n",
    "        \n",
    "        cur_value    = value_network.estimate(states)\n",
    "        next_value   = value_network.estimate(next_states)\n",
    "        \n",
    "        values       = rewards + gamma * cur_value\n",
    "        values_list.append(values)\n",
    "        advantages   = rewards + gamma * next_value - cur_value\n",
    "        advantages_list.append(advantages)\n",
    "        \n",
    "        if np.any(env_info.local_done):\n",
    "            break\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "    return {\n",
    "        \"log_probs\"  : torch.cat(prob_list).reshape(n_agents, t_max, -1),\n",
    "        \"states\"     : np.array(state_list).reshape(n_agents, t_max, -1), \n",
    "        \"actions\"    : np.array(action_list).reshape(n_agents, t_max, -1),\n",
    "        \"rewards\"    : torch.cat(reward_list).reshape(n_agents, t_max), \n",
    "        \"values\"     : torch.cat(values_list).reshape(n_agents, t_max), \n",
    "        \"advantages\" : torch.cat(advantages_list).reshape(n_agents, t_max),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_loss(old_probs, states, R, epsilon=0.3):\n",
    "    \"\"\"\n",
    "        Loss as defined in PPO algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        old_probs: torch.Tensor\n",
    "        Tensor that defines log probabilities of actions under evaluated policy\n",
    "        states: np.Array [n_agents x n_episoded x state_size] \n",
    "        signal: np.Array [n_agents x n_episoded x state_size] \n",
    "    \"\"\"\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    _, new_probs = policy_network.act(states)\n",
    "    new_probs = new_probs.reshape(old_probs.shape)        \n",
    "    ratio = (new_probs/old_probs)    \n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    return (-torch.min(ratio, clip)  * torch.mean(R)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_loss(log_probs, R):\n",
    "    \"\"\"\n",
    "        Loss as defined in REINFORCE algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        log_probs: torch.Tensor\n",
    "        Tensor that defines compute graph for action log probabilities\n",
    "        R: torch.Tensor\n",
    "        Tensor that defines rewards along each episode in trajectory\n",
    "    \"\"\"\n",
    "    R = torch.from_numpy(R).float().to(device)\n",
    "    return (-log_probs * torch.mean(R)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a3c(n_episodes=1000, max_t=2000, print_every=100, n_pp_iterations=3, flavor='PPO'):\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=n_episodes).start()\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        trajectory = unroll_trajectory(env, brain_name, num_agents, max_t)\n",
    "\n",
    "        mean_rewards = trajectory['rewards'].sum(1).mean().detach().cpu().numpy()\n",
    "        scores_deque.append(mean_rewards)\n",
    "        scores.append(mean_rewards)\n",
    "        \n",
    "        # Modification 2: Use Future Rewards\n",
    "        R = trajectory['rewards'].detach().cpu().numpy()\n",
    "        R = np.flip(np.flip(R).cumsum(1)).reshape(num_agents, max_t).copy()\n",
    "        A = trajectory['advantages'].detach().cpu().numpy().reshape(R.shape)\n",
    "                \n",
    "        # Modification 1: Batch Normalization of Reward Signal\n",
    "        if R.shape[0] > 10:\n",
    "            R_mean = np.mean(R, axis=0)\n",
    "            R_std = np.std(R, axis=0)\n",
    "            R = np.nan_to_num((R - R_mean) / R_std)\n",
    "            \n",
    "        if A.shape[0] > 10:\n",
    "            A_mean = np.mean(A, axis=0)\n",
    "            A_std = np.std(A, axis=0)\n",
    "            A = np.nan_to_num((A - A_mean) / A_std)\n",
    "            \n",
    "        if flavor == 'REINFORCE':\n",
    "            policy_loss = reinforce_loss(trajectory['log_probs'], R)\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "            \n",
    "        if flavor == \"PPO\":\n",
    "            states = torch.from_numpy(trajectory['states']).float().to(device)\n",
    "            value_loss = value_criterion(\n",
    "                value_network(states).reshape(num_agents, max_t), \n",
    "                trajectory['values']\n",
    "            )\n",
    "            value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "            \n",
    "            signal = torch.from_numpy((R-A)).float().to(device)\n",
    "            for i in range(n_pp_iterations):\n",
    "                policy_loss = ppo_loss(trajectory['log_probs'], states, signal)\n",
    "                policy_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                policy_optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "        if np.mean(scores_deque)>=30.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "            \n",
    "        timer.update(i_episode)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "D:\\Anaconda3\\envs\\gym\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n",
      "training loop:   0% |                                           | ETA:  5:18:27\r"
     ]
    }
   ],
   "source": [
    "policy_network   = Policy(state_size=state_size, action_size=action_size).to(device)\n",
    "policy_optimizer = optim.Adam(policy_network.parameters(), lr=1e-4)\n",
    "\n",
    "value_network    = Value(state_size=state_size, action_size=1).to(device)\n",
    "value_criterion  = nn.MSELoss()\n",
    "value_optimizer  = optim.Adam(value_network.parameters(), lr=1e-4)\n",
    "\n",
    "%time scores = a3c(n_episodes=1000, max_t=1000, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
