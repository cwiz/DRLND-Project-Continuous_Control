{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# 1 agent\n",
    "env = UnityEnvironment(file_name=\"./Reacher_Windows_x86_64_1_agent/Reacher.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = Variable(torch.FloatTensor([math.pi]), requires_grad=True).cuda()\n",
    "\n",
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(Variable(x, requires_grad=True)-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
    "    return a*b\n",
    "\n",
    "def probs_to_actions(mu, sigma_sq):\n",
    "    eps = torch.randn(mu.size())\n",
    "    sigma_sq = torch.ones(mu.size()).cuda()\n",
    "    action = (mu + sigma_sq.sqrt()*Variable(eps, requires_grad=True).cuda())\n",
    "    prob = normal(action, mu, sigma_sq)\n",
    "    \n",
    "    return action.cpu().reshape(-1, 4).reshape(-1, 4), torch.log(prob).reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1_size=256, fc2_size=128):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(33, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        \n",
    "        self.fc3_mu = nn.Linear(fc2_size, action_size)\n",
    "        self.fc3_sigma_squared = nn.Linear(fc2_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        mu = F.hardtanh(self.fc3_mu(x))\n",
    "        sigma = F.relu(self.fc3_sigma_squared(x))\n",
    "        \n",
    "        return mu, sigma\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        mu, sigma = self.forward(state)\n",
    "        return probs_to_actions(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    \"\"\"\n",
    "        Critic network that estimates a Value Function used as a baseline\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, fc1_size=128, fc2_size=64):\n",
    "        super(Value, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(33, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        return self.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_trajectory(env, brain_name, policy, t_max=1000, gamma=0.995):\n",
    "    \"\"\"\n",
    "        Unroll a trajectory and return \n",
    "        list of action probabilities, states, actions and rewards\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: unityagents.UnityEnvironment\n",
    "        Environment to play on.\n",
    "        brain_name: String\n",
    "        Name of brain used for UnityEnvironment\n",
    "        policy: torch.nn.Module\n",
    "        A neural network mapping states to action probabilities \n",
    "        The action to take at a given state\n",
    "        t_max: int\n",
    "        Maximum number of episodes in trajectory\n",
    "    \"\"\"\n",
    "    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    # num_agents = len(env.ps)\n",
    "\n",
    "    state_list      = []\n",
    "    reward_list     = []\n",
    "    prob_list       = []\n",
    "    action_list     = []\n",
    "    values_list     = []\n",
    "    advantages_list = []\n",
    "\n",
    "    states = env_info.vector_observations\n",
    "    \n",
    "    for _ in range(t_max):\n",
    "\n",
    "        a, p = policy_network.act(states)\n",
    "        actions = a.cpu().detach().numpy()\n",
    "        action_probabilities = p.cpu().detach().numpy()\n",
    "        \n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        \n",
    "        prob_list.append(action_probabilities)\n",
    "        state_list.append(states)\n",
    "        action_list.append(actions)\n",
    "        \n",
    "        rewards = env_info.rewards\n",
    "        reward_list.append(env_info.rewards)\n",
    "        \n",
    "        cur_value    = value_network.estimate(states).cpu().detach().numpy()\n",
    "        next_value   = value_network.estimate(next_states).cpu().detach().numpy()\n",
    "        \n",
    "        values       = rewards + gamma * cur_value\n",
    "        values_list.append(values)\n",
    "        advantages   = rewards + gamma * next_value - cur_value\n",
    "        advantages_list.append(advantages)\n",
    "        \n",
    "        if np.any(env_info.local_done):\n",
    "            break\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "    return {\n",
    "        \"log_probs\"  : np.array(prob_list), \n",
    "        \"states\"     : np.array(state_list), \n",
    "        \"actions\"    : np.array(action_list), \n",
    "        \"rewards\"    : np.array(reward_list), \n",
    "        \"values\"     : np.array(values_list), \n",
    "        \"advantages\" : np.array(advantages_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate(old_probs, states, rewards, advantages, epsilon=0.2):\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    _, new_probs = policy_network.act(states)\n",
    "    new_probs = new_probs.reshape(old_probs.shape)        \n",
    "    ratio = (new_probs/old_probs)\n",
    "    pl = []\n",
    "    for i, log_prob in enumerate(ratio):\n",
    "        R = np.mean(rewards[:,i])\n",
    "        A = np.mean(advantages[i])\n",
    "        clip = torch.clamp(ratio[i], 1-epsilon, 1+epsilon)\n",
    "        pl.append(torch.min(ratio[i] * R,  clip * R) - A)\n",
    "    return torch.cat(pl).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a3c(n_episodes=300, max_t=1000, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "       \n",
    "        trajectory = unroll_trajectory(env, brain_name, max_t)\n",
    "\n",
    "        mean_rewards = np.mean(np.sum(trajectory['rewards'], axis=0))\n",
    "        scores_deque.append(mean_rewards)\n",
    "        scores.append(mean_rewards)\n",
    "        \n",
    "        # Modification 2: Use Future Rewards\n",
    "        R = np.flip(np.flip(trajectory['rewards']).cumsum(axis=1)).reshape(-1, max_t).copy()\n",
    "                \n",
    "        # Modification 1: Batch Normalization of Reward Signal\n",
    "        if R.shape[0] > 10:\n",
    "            R_mean = np.mean(R, axis=0)\n",
    "            R_std = np.std(R, axis=0) + 1.0e-10\n",
    "            R = np.nan_to_num((R - R_mean) / R_std)\n",
    "            \n",
    "        # Modification 4: Value-function baseline\n",
    "        for i in range(trajectory['states'].shape[0]):\n",
    "            state = trajectory['states'][i]\n",
    "            value = trajectory['values'][i]\n",
    "            estimated_value = value_network.estimate(state)\n",
    "            value = torch.from_numpy(value).float().to(device)\n",
    "            value_loss = value_criterion(estimated_value, value)\n",
    "            value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "        \n",
    "        # Modification 3: PPO\n",
    "        for i in range(0, 3):\n",
    "            policy_loss = -surrogate(trajectory['log_probs'], trajectory['states'], R, trajectory['advantages'])\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "            \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "        if np.mean(scores_deque)>=30.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network   = Policy(state_size=33, action_size=action_size).to(device)\n",
    "policy_optimizer = optim.Adam(policy_network.parameters(), lr=1e-4)\n",
    "\n",
    "value_network    = Value(state_size=33).to(device)\n",
    "value_criterion  = nn.MSELoss()\n",
    "value_optimizer  = optim.Adam(value_network.parameters(), lr=1e-4)\n",
    "\n",
    "%time scores = a3c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
