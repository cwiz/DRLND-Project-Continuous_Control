{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Reacher_Windows_x86_64_1_agent/Reacher.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1_size=128, fc2_size=64, log_std=-0.5):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        \n",
    "        self.action_mean = nn.Linear(fc2_size, action_size)\n",
    "        self.action_mean.weight.data.mul_(0.1)\n",
    "        self.action_mean.bias.data.mul_(0.0)\n",
    "\n",
    "        self.action_log_std = nn.Parameter(torch.ones(1, action_size) * log_std)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        action_mean = self.action_mean(x)\n",
    "        action_log_std = self.action_log_std.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_log_std)\n",
    "\n",
    "        return action_mean, action_log_std, action_std\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action_mean, action_log_std, action_std = self.forward(state)\n",
    "        action = torch.clamp(torch.normal(action_mean, action_std), -1, 1)\n",
    "        \n",
    "        return action, action_log_std\n",
    "    \n",
    "class Value(nn.Module):\n",
    "    \"\"\"\n",
    "        Critic network that estimates a Value Function used as a baseline\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size=1, fc1_size=128, fc2_size=64):\n",
    "        super(Value, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        return self.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network   = Policy(state_size=state_size, action_size=action_size).to(device)\n",
    "policy_optimizer = optim.Adam(policy_network.parameters(), lr=5e-4)\n",
    "\n",
    "value_network    = Value(state_size=state_size, action_size=1).to(device)\n",
    "value_criterion  = nn.MSELoss()\n",
    "value_optimizer  = optim.Adam(value_network.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import unroll_trajectory, rewards_to_go\n",
    "\n",
    "def interact(actions):\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    return env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "\n",
    "def reset():\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    return env_info.vector_observations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar as pb\n",
    "from collections import deque\n",
    "\n",
    "def advantage_estimate(states, rewards, next_states, value_network, n_agents, t_max, gamma=0.995):\n",
    "    current_values = value_network.estimate(states).reshape(n_agents, t_max).detach().cpu().numpy()\n",
    "    next_values = value_network.estimate(next_states).reshape(n_agents, t_max).detach().cpu().numpy()\n",
    "    rewards = rewards.reshape(n_agents, t_max)\n",
    "    \n",
    "    advantages = rewards + gamma * np.nan_to_num(next_values) - current_values\n",
    "    return advantages\n",
    "\n",
    "def ppo_clip_objective(policy_network, states, advantages, old_log_probs, epsilon=0.1):\n",
    "    \n",
    "    _, new_log_probs = policy_network.act(states)\n",
    "    ratio = new_log_probs / old_log_probs.detach()\n",
    "    advantages = torch.from_numpy(advantages).float().to(device)\n",
    "    \n",
    "    min_term = torch.min(\n",
    "        ratio*advantages[:,:,None], \n",
    "        torch.clamp(ratio, 1-epsilon, 1+epsilon)*advantages[:,:,None]\n",
    "    ).mean(1).mean(0).mean()\n",
    "    \n",
    "    return min_term\n",
    "\n",
    "def ppo_clip(t_max, n_episodes=1000, print_every=100, n_policy_iterations=20, n_value_iterations=20):\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=n_episodes).start()\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        \n",
    "        trajectory = unroll_trajectory(interact, reset, policy_network, num_agents, t_max=t_max)\n",
    "        \n",
    "        mean_rewards = trajectory['rewards'].sum(axis=1).mean()\n",
    "        scores_deque.append(mean_rewards)\n",
    "        scores.append(mean_rewards)\n",
    "        \n",
    "        states    = trajectory['states']\n",
    "        rewards   = rewards_to_go(trajectory['rewards'])\n",
    "        log_probs = trajectory['log_probs']\n",
    "        \n",
    "        next_states = np.roll(states, -1, axis=2)\n",
    "        next_states[:,-1,:] = np.nan\n",
    "\n",
    "        advantages = advantage_estimate(states, rewards, next_states, value_network, num_agents, t_max)\n",
    "        \n",
    "        for i in range(0, n_policy_iterations):\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss = -ppo_clip_objective(policy_network, states, advantages, log_probs)\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "            \n",
    "        for i in range(0, n_value_iterations):\n",
    "            value_optimizer.zero_grad()\n",
    "            target = value_network.estimate(states).reshape(num_agents, -1)\n",
    "            objective = torch.from_numpy(rewards).float().to(device).reshape(num_agents, -1)\n",
    "            value_loss = value_criterion(target, objective)\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "            \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "        if np.mean(scores_deque)>=30.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "            \n",
    "        timer.update(i_episode)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   3% |#                                          | ETA:  3:01:58\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   6% |##                                         | ETA:  2:54:05\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200\tAverage Score: 0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  10% |####                                       | ETA:  2:47:19\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300\tAverage Score: 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                      | ETA:  2:40:55\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400\tAverage Score: 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  16% |#######                                    | ETA:  2:34:34\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500\tAverage Score: 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                   | ETA:  2:28:17\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 600\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  23% |##########                                 | ETA:  2:22:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 700\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  26% |###########                                | ETA:  2:15:46\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  30% |############                               | ETA:  2:09:34\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 900\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  33% |##############                             | ETA:  2:03:21\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  36% |###############                            | ETA:  1:57:07\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  40% |#################                          | ETA:  1:50:55\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  43% |##################                         | ETA:  1:44:44\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1300\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  46% |####################                       | ETA:  1:38:33\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1400\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  50% |#####################                      | ETA:  1:32:22\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  53% |######################                     | ETA:  1:26:12\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  56% |########################                   | ETA:  1:20:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1700\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  60% |#########################                  | ETA:  1:13:53\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  63% |###########################                | ETA:  1:07:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  66% |############################               | ETA:  1:01:34\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  70% |##############################             | ETA:  0:55:24\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2100\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  73% |###############################            | ETA:  0:49:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2200\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  76% |################################           | ETA:  0:43:05\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2300\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  80% |##################################         | ETA:  0:36:55\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2400\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  83% |###################################        | ETA:  0:30:46\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  86% |#####################################      | ETA:  0:24:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2600\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  90% |######################################     | ETA:  0:18:27\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2700\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  93% |########################################   | ETA:  0:12:18\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2800\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  96% |#########################################  | ETA:  0:06:09\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2900\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  99% |########################################## | ETA:  0:00:03\r"
     ]
    }
   ],
   "source": [
    "scores = ppo_clip(t_max=1000, n_episodes=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
