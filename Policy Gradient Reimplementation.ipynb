{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Reacher_Windows_x86_64_1_agent/Reacher.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = Variable(torch.FloatTensor([math.pi]), requires_grad=True).cuda()\n",
    "\n",
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(Variable(x, requires_grad=True)-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
    "    return a*b\n",
    "\n",
    "def probs_to_actions(mu, sigma_sq):\n",
    "    eps = torch.randn(mu.size())\n",
    "    sigma_sq = torch.ones(mu.size()).cuda()\n",
    "    action = (mu + sigma_sq.sqrt()*Variable(eps, requires_grad=True).cuda())\n",
    "    prob = normal(action, mu, sigma_sq)\n",
    "    \n",
    "    return action.reshape(-1, 4), torch.log(prob).reshape(-1, 4)\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size, n_agents=1, fc1_size=128, fc2_size=64):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        \n",
    "        self.fc3_mu = nn.Linear(fc2_size, action_size)\n",
    "        self.fc3_sigma_squared = nn.Linear(fc2_size, action_size)\n",
    "        \n",
    "        self.n_agents = n_agents\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        mu = torch.tanh(self.fc3_mu(x))\n",
    "        sigma = torch.sigmoid(self.fc3_sigma_squared(x))\n",
    "        \n",
    "        return mu, sigma\n",
    "    \n",
    "    def act(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "        mu, sigma = self.forward(state)\n",
    "        return probs_to_actions(mu, sigma)\n",
    "    \n",
    "class Value(nn.Module):\n",
    "    \"\"\"\n",
    "        Critic network that estimates a Value Function used as a baseline\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size=1, n_agents=1, fc1_size=128, fc2_size=64):\n",
    "        super(Value, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        return self.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network   = Policy(state_size=state_size, action_size=action_size).to(device)\n",
    "policy_optimizer = optim.Adam(policy_network.parameters(), lr=1e-3)\n",
    "\n",
    "value_network    = Value(state_size=state_size, action_size=1).to(device)\n",
    "value_criterion  = nn.MSELoss()\n",
    "value_optimizer  = optim.Adam(value_network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import unroll_trajectory, rewards_to_go\n",
    "\n",
    "def interact(actions):\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    return env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "\n",
    "def reset():\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    return env_info.vector_observations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar as pb\n",
    "from collections import deque\n",
    "\n",
    "def advantage_estimate(states, rewards, next_states, value_network, n_agents, t_max, gamma=0.995):\n",
    "    current_values = value_network.estimate(states).reshape(n_agents, t_max).detach().cpu().numpy()\n",
    "    next_values = value_network.estimate(next_states).reshape(n_agents, t_max).detach().cpu().numpy()\n",
    "    rewards = rewards.reshape(n_agents, t_max)\n",
    "    \n",
    "    advantages = rewards + gamma * np.nan_to_num(next_values) - current_values\n",
    "    return advantages\n",
    "\n",
    "def ppo_clip_objective(policy_network, states, advantages, old_log_probs, epsilon=0.1):\n",
    "    \n",
    "    old_log_probs = old_log_probs.detach()\n",
    "    _, new_log_probs = policy_network.act(states)\n",
    "    ratio = new_log_probs / old_log_probs\n",
    "    advantages = torch.from_numpy(advantages).float().to(device)\n",
    "    \n",
    "    min_term = torch.min(\n",
    "        ratio*advantages[:,:,None], \n",
    "        torch.clamp(ratio, 1-epsilon, 1+epsilon)*advantages[:,:,None]\n",
    "    ).mean(1).mean(0).mean()\n",
    "    \n",
    "    return min_term\n",
    "\n",
    "def ppo_clip(t_max, n_episodes=1000, print_every=10, n_policy_iterations=10, n_value_iterations=1000):\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=n_episodes).start()\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        \n",
    "        trajectory = unroll_trajectory(interact, reset, policy_network, num_agents, t_max=t_max)\n",
    "        \n",
    "        mean_rewards = trajectory['rewards'].sum(axis=1).mean()\n",
    "        scores_deque.append(mean_rewards)\n",
    "        scores.append(mean_rewards)\n",
    "        \n",
    "        states    = trajectory['states']\n",
    "        rewards   = rewards_to_go(trajectory['rewards'])\n",
    "        log_probs = trajectory['log_probs']\n",
    "        \n",
    "        next_states = np.roll(states, -1, axis=2)\n",
    "        next_states[:,-1,:] = np.nan\n",
    "\n",
    "        advantages = advantage_estimate(states, rewards, next_states, value_network, num_agents, t_max)\n",
    "        \n",
    "        for i in range(0, n_policy_iterations):\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss = -ppo_clip_objective(policy_network, states, advantages, log_probs)\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "            \n",
    "        for i in range(0, n_value_iterations):\n",
    "            value_optimizer.zero_grad()\n",
    "            target = value_network.estimate(states).reshape(num_agents, -1)\n",
    "            objective = torch.from_numpy(rewards).float().to(device).reshape(num_agents, -1)\n",
    "            value_loss = value_criterion(target, objective)\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "            \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "        if np.mean(scores_deque)>=30.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "            \n",
    "        timer.update(i_episode)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   3% |#                                          | ETA:  0:39:39\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   6% |##                                         | ETA:  0:36:49\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAverage Score: 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  10% |####                                       | ETA:  0:34:57\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30\tAverage Score: 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                      | ETA:  0:33:25\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40\tAverage Score: 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  16% |#######                                    | ETA:  0:31:55\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\tAverage Score: 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |########                                   | ETA:  0:30:31\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60\tAverage Score: 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  23% |##########                                 | ETA:  0:29:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 70\tAverage Score: 0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  26% |###########                                | ETA:  0:27:48\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80\tAverage Score: 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  30% |############                               | ETA:  0:26:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 90\tAverage Score: 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  33% |##############                             | ETA:  0:25:08\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  36% |###############                            | ETA:  0:23:49\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 110\tAverage Score: 0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  40% |#################                          | ETA:  0:22:32\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 120\tAverage Score: 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  43% |##################                         | ETA:  0:21:15\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 130\tAverage Score: 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  46% |####################                       | ETA:  0:19:59\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 140\tAverage Score: 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  50% |#####################                      | ETA:  0:18:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 150\tAverage Score: 0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  53% |######################                     | ETA:  0:17:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 160\tAverage Score: 0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  56% |########################                   | ETA:  0:16:13\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 170\tAverage Score: 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  60% |#########################                  | ETA:  0:14:58\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 180\tAverage Score: 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  63% |###########################                | ETA:  0:13:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 190\tAverage Score: 0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  66% |############################               | ETA:  0:12:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200\tAverage Score: 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  70% |##############################             | ETA:  0:11:13\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 210\tAverage Score: 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  73% |###############################            | ETA:  0:09:58\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 220\tAverage Score: 0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  76% |################################           | ETA:  0:08:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 230\tAverage Score: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  80% |##################################         | ETA:  0:07:28\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 240\tAverage Score: 0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  83% |###################################        | ETA:  0:06:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250\tAverage Score: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  86% |#####################################      | ETA:  0:04:59\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 260\tAverage Score: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  90% |######################################     | ETA:  0:03:44\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 270\tAverage Score: 0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  93% |########################################   | ETA:  0:02:29\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 280\tAverage Score: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  96% |#########################################  | ETA:  0:01:14\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 290\tAverage Score: 0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  99% |########################################## | ETA:  0:00:07\r"
     ]
    }
   ],
   "source": [
    "scores = ppo_clip(t_max=300, n_episodes=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
