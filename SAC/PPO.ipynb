{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.distributions import normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('LunarLanderContinuous-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 1\n",
    "action_size = env.action_space.shape[0]\n",
    "state_size = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal\n",
    "# https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c_ppo_acktr/distributions.py\n",
    "# FixedNormal = torch.distributions.Normal\n",
    "\n",
    "# log_prob_normal = FixedNormal.log_prob\n",
    "# FixedNormal.log_probs = lambda self, actions: log_prob_normal(self, actions).sum(-1, keepdim=True)\n",
    "\n",
    "# normal_entropy = FixedNormal.entropy\n",
    "# FixedNormal.entropy = lambda self: normal_entropy(self).sum(-1)\n",
    "# FixedNormal.mode = lambda self: self.mean\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size=1, n_agents=1, fc1_size=256, fc2_size=256):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3_mu = nn.Linear(fc2_size, action_size)\n",
    "        self.fc3_sigma = nn.Linear(fc2_size, action_size)\n",
    "        \n",
    "        self.fc3_mu.bias.data.fill_(0.3)\n",
    "        self.fc3_mu.weight.data.fill_(0.1)\n",
    "        \n",
    "        self.fc3_sigma.bias.data.fill_(0.5)\n",
    "        self.fc3_sigma.weight.data.fill_(0.3)\n",
    "\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        mu = torch.tanh(self.fc3_mu(x))\n",
    "        sigma = torch.sigmoid(self.fc3_sigma(x))\n",
    "        \n",
    "        return Normal(mu, 0.01)\n",
    "    \n",
    "class Value(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size=1, n_agents=1, fc1_size=400, fc2_size=300):\n",
    "        \n",
    "        super(Value, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact(action):\n",
    "    next_state, reward, done, _ = env.step(action.reshape(action_size))\n",
    "#     env.render()\n",
    "    return next_state.reshape(num_agents, -1), np.array(reward).reshape(num_agents, -1), np.array(done).reshape(num_agents, -1)\n",
    "\n",
    "def reset():\n",
    "    state = env.reset()\n",
    "    return state.reshape(num_agents, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'q_network'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b008c239bb40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mvalue_network\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mValue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mn_agents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'q_network'"
     ]
    }
   ],
   "source": [
    "from agent import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=state_size, \n",
    "    action_size=action_size,\n",
    "    policy_network=Policy,\n",
    "    value_network=Value,\n",
    "    n_agents=num_agents, \n",
    "    device=device,\n",
    ")\n",
    "\n",
    "def run(fn_interact, fn_reset, agent):\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=200)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        states = fn_reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            actions = agent.act(states)\n",
    "            \n",
    "            next_states, rewards, dones = fn_interact(actions)\n",
    "            \n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "        timer.update(i_episode)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time scores = run(t_max=300, n_episodes=int(1000), print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
